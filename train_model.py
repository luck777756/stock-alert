import os, time, random, logging
import numpy as np
import pandas as pd
import yfinance as yf
import joblib
from xgboost import XGBClassifier
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from ta.volatility import BollingerBands
import shutil

# ë¡œê·¸ ì„¤ì •
d_logging = logging.basicConfig
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

# ìºì‹œ ë””ë ‰í† ë¦¬\CACHE_DIR = 'data_cache'
os.makedirs(CACHE_DIR, exist_ok=True)

def load_hist(ticker, base_sleep=0.1, max_retry=3):
    """
    Adaptive backoffê³¼ ë¡œì»¬ ìºì‹œë¥¼ í™œìš©í•˜ì—¬ Yahoo Finance ë°ì´í„° ë¡œë“œ
    ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„  ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
    """
    path = os.path.join(CACHE_DIR, f"{ticker}.csv")
    # ìºì‹œ ë¡œë“œ
    if os.path.exists(path):
        try:
            df = pd.read_csv(path, index_col=0, parse_dates=True)
            logging.info(f"{ticker}: ìºì‹œ ë¡œë“œ ì„±ê³µ")
            return df
        except Exception as e:
            logging.error(f"{ticker}: ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # adaptive backoff retry
    for i in range(max_retry):
        try:
            df = yf.download(
                ticker,
                period="1y",
                interval="1d",
                auto_adjust=True,
                threads=True,
                group_by='ticker'
            )
            df.dropna(inplace=True)
            if not df.empty:
                df.to_csv(path)
                logging.info(f"{ticker}: ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ì €ì¥ ì„±ê³µ")
                return df
        except Exception as e:
            logging.error(f"{ticker}: ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({i+1}/{max_retry}): {e}")
        time.sleep(base_sleep * (2**i) + random.random()*0.1)
    logging.error(f"{ticker}: ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨ after {max_retry} retries")
    return None


def make_features(df):
    X = pd.DataFrame(index=df.index)
    X['obv'] = ((df['Close'].diff() > 0) * df['Volume'] - (df['Close'].diff() < 0) * df['Volume']).cumsum()
    X['vol_pct'] = df['Volume'].pct_change().fillna(0)
    X['ma20_diff'] = (df['Close'] - df['Close'].rolling(20).mean()) / df['Close'].rolling(20).mean()
    bb = BollingerBands(df['Close'], window=20, window_dev=2)
    pb = bb.bollinger_pband().fillna(0)
    arr = pb.values
    if arr.ndim > 1:
        arr = arr.flatten()
    X['bb_pctb'] = pd.Series(arr, index=df.index)
    X['adx'] = 0
    return X.dropna()


def label_future(df, days=10, target=0.6):
    fut = df['Close'].shift(-days)
    ret = fut / df['Close'] - 1
    labels = (ret >= target).astype(int)
    return labels.dropna()


if __name__ == '__main__':
    # í‹°ì»¤ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    try:
        with open("tickers_nasdaq.txt") as f:
            tickers = [t.strip() for t in f if t.strip()]
    except FileNotFoundError:
        logging.error("tickers_nasdaq.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)

    all_X, all_y = [], []
    for t in tickers:
        df = load_hist(t)
        if df is None or len(df) < 60:
            continue
        X = make_features(df)
        y = label_future(df, days=10, target=0.6)
        # í”¼ì²˜-ë ˆì´ë¸” ì¼ì¹˜í•˜ë„ë¡ êµì§‘í•© ì¸ë±ìŠ¤ ì‚¬ìš©
        idx = X.index.intersection(y.index)
        all_X.append(X.loc[idx])
        all_y.append(y.loc[idx])

    if not all_X:
        logging.error("ìœ íš¨í•œ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)

    # ë°ì´í„° ë³‘í•© ë° ì •ë ¬
    X_full = pd.concat(all_X).sort_index()
    y_full = pd.concat(all_y).sort_index()

    # ëª¨ë¸ í•™ìŠµ
    tscv = TimeSeriesSplit(n_splits=5)
    scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}
    params = {
        'n_estimators': [50, 100],
        'max_depth': [3, 5],
        'learning_rate': [0.01, 0.1]
    }
    clf = GridSearchCV(
        XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
        params,
        cv=tscv,
        scoring=scoring,
        refit='precision',
        return_train_score=False,
        n_jobs=-1
    )
    clf.fit(X_full, y_full)
    logging.info(f"Best Precision: {clf.best_score_:.4f} with params {clf.best_params_}")

    # ëª¨ë¸ ì €ì¥ ë° ì••ì¶•
    model_path = "best_model.pkl"
    joblib.dump(clf.best_estimator_, model_path)
    shutil.make_archive("trained_model", 'zip', '.', model_path)
    logging.info("ğŸ“¦ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° trained_model.zip ìƒì„±ë¨")
